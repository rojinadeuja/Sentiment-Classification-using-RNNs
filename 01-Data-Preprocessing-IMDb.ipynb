{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
       "1           1  test  This is an example of why the majority of acti...   neg   \n",
       "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
       "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
       "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
       "\n",
       "          file  \n",
       "0      0_2.txt  \n",
       "1  10000_4.txt  \n",
       "2  10001_1.txt  \n",
       "3  10002_3.txt  \n",
       "4  10003_3.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/imdb_master.csv', encoding=\"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Unnecssary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the data frame:  (50000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                             review  sentiment\n",
       "0  test  Once again Mr. Costner has dragged out a movie...          0\n",
       "1  test  This is an example of why the majority of acti...          0\n",
       "2  test  First of all I hate those moronic rappers, who...          0\n",
       "3  test  Not even the Beatles could write songs everyon...          0\n",
       "4  test  Brass pictures (movies is not a fitting word f...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['Unnamed: 0','file'], axis=1)\n",
    "df.columns = [\"type\", \"review\",\"sentiment\"]\n",
    "\n",
    "df = df[df.sentiment != 'unsup']\n",
    "df['sentiment'] = df['sentiment'].map({'pos': 1, 'neg': 0})\n",
    "\n",
    "print(\"Dimension of the data frame: \", df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting a list of sentences to a list of lists containing tokenized words\n",
    "def docs_preprocessor(inputDocs):\n",
    "    docs = inputDocs.copy()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenize the words.\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    #docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Convert a list of sentences to a list of lists containing tokenized words\n",
    "texts_tokenized = docs_preprocessor(df[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bigrams/Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Add bigrams to docs (only ones that appear 10 times or more).\n",
    "'''\n",
    "bigram = Phrases(texts_tokenized, min_count=10, threshold=0.5, scoring='npmi')\n",
    "#trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(texts_tokenized)):\n",
    "    for token in bigram[texts_tokenized[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            texts_tokenized[idx].append(token)\n",
    "#     for token in trigram[texts_tokenized[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_tokenized[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary size:  98722\n"
     ]
    }
   ],
   "source": [
    "texts_tokenized_counts = defaultdict(int)\n",
    "\n",
    "for row in texts_tokenized:\n",
    "    for word in row:\n",
    "            texts_tokenized_counts[word] += 1\n",
    "vocabulary = list(texts_tokenized_counts.keys())\n",
    "\n",
    "print(\"\\nVocabulary size: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases Count\n",
    "\n",
    "We can investigate the phrases (by uncommenting the print statement below) and, if needed, increase/decrease the number of phrases by varying the \"min_count\" and \"threshold\" parameters of the Phrases object (above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number of Phrases:  5401\n"
     ]
    }
   ],
   "source": [
    "# Find phrases that are joined by an underscore (_)\n",
    "numOfPhrases = 0\n",
    "for i in range(len(vocabulary)):\n",
    "    if(vocabulary[i].find(\"_\") > -1):\n",
    "        numOfPhrases += 1\n",
    "        #print(vocabulary[i])\n",
    "    \n",
    "print(\"\\nTotal Number of Phrases: \", numOfPhrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Tokens Count\n",
    "The Keras tokenizer needs us to set the number of high frequency tokens/words. These top k tokens will be used to define the length of the feature vectors.\n",
    "\n",
    "Thus, we need to understand how many words have high frequency. Analyzing the frequency of the tokens we can decide the threshold for the high frequency tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens (including repetition):  11633232\n"
     ]
    }
   ],
   "source": [
    "#Create a list of ALL tokens\n",
    "tokens = []\n",
    "for text in texts_tokenized:\n",
    "    for token in text:\n",
    "        tokens.append(token)\n",
    "        \n",
    "print(\"Total number of tokens (including repetition): \", len(tokens))\n",
    "\n",
    "\n",
    "# Sort tokens from high to low frequency\n",
    "token_frequency = Counter(tokens)\n",
    "token_frequency_ordered = OrderedDict(sorted(token_frequency.items(), key=lambda t: t[1], reverse = True))\n",
    "\n",
    "\n",
    "'''\n",
    "We may view the top frequent tokens (by uncommenting the print statement below).\n",
    "By varying the \"top_k_tokens\" we can see the top k tokens.\n",
    "This will help to determine the threshold for top k frequent tokens to create the vectors.\n",
    "'''\n",
    "numOfTokens = 0\n",
    "top_k_tokens = 10000\n",
    "for k, v in token_frequency_ordered.items(): \n",
    "    if k in token_frequency_ordered.keys():\n",
    "        numOfTokens += 1\n",
    "        #print(k, v)\n",
    "        if(numOfTokens == top_k_tokens):\n",
    "            break# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove underscores from Phrases\n",
    "\n",
    "The bigrams are created by adding an underscore between two words.\n",
    "\n",
    "We remove the underscore from all words. Otherwise later the Keras tokenizer will split the phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts_tokenized)):\n",
    "    for j in range(len(texts_tokenized[i])):\n",
    "        texts_tokenized[i][j] = texts_tokenized[i][j].replace(\"_\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text corpus by adding Phrases\n",
    "\n",
    "Combine the words, including the phrases to create a full text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_processed = []\n",
    "\n",
    "for i in range(len(texts_tokenized)):\n",
    "    text = \" \".join(texts_tokenized[i])\n",
    "    texts_processed.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['once again mr costner ha dragged out movie for far longer than necessary aside from the terrific sea rescue sequence of which there are very few just did not care about any of the character most of u have ghost in the closet and costner character are realized early on and then forgotten until much later by which time did not care the character we should really care about is very cocky overconfident ashton kutcher the problem is he come off a kid who think he better than anyone else around him and show no sign of cluttered closet his only obstacle appears to be winning over costner finally when we are well past the half way point of this stinker costner tell u all about kutcher ghost we are told why kutcher is driven to be the best with no prior inkling or foreshadowing no magic here it wa all could do to keep from turning it off an hour in onceagain ashtonkutcher betterthan anyoneelse',\n",
       " 'this is an example of why the majority of action film are the same generic and boring there really nothing worth watching here complete waste of the then barely tapped talent of ice and ice cube who ve each proven many time over that they are capable of acting and acting well don bother with this one go see new jack city ricochet or watch new york undercover for ice or boyz the hood higher learning or friday for ice cube and see the real deal ice horribly cliched dialogue alone make this film grate at the teeth and still wondering what the heck bill paxton wa doing in this film and why the heck doe he always play the exact same character from alien onward every film ve seen with bill paxton ha him playing the exact same irritating character and at least in alien his character died which made it somewhat gratifying br br overall this is second rate action trash there are countless better film to see and if you really want to see this one watch judgement night which is practically carbon copy but ha better acting and better script the only thing that made this at all worth watching wa decent hand on the camera the cinematography wa almost refreshing which come close to making up for the horrible film itself but not quite 10 worthwatching completewaste icecube newyork higherlearning icecube billpaxton exactsame veseen billpaxton exactsame atleast brbr ifyou carboncopy worthwatching']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first two reviews in the text corpus\n",
    "texts_processed[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add processed Reviews from Corpus into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Processed_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>0</td>\n",
       "      <td>once again mr costner ha dragged out movie for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>0</td>\n",
       "      <td>this is an example of why the majority of acti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>0</td>\n",
       "      <td>first of all hate those moronic rapper who cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>0</td>\n",
       "      <td>not even the beatles could write song everyone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>0</td>\n",
       "      <td>brass picture movie is not fitting word for th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                             review  sentiment  \\\n",
       "0  test  Once again Mr. Costner has dragged out a movie...          0   \n",
       "1  test  This is an example of why the majority of acti...          0   \n",
       "2  test  First of all I hate those moronic rappers, who...          0   \n",
       "3  test  Not even the Beatles could write songs everyon...          0   \n",
       "4  test  Brass pictures (movies is not a fitting word f...          0   \n",
       "\n",
       "                                   Processed_Reviews  \n",
       "0  once again mr costner ha dragged out movie for...  \n",
       "1  this is an example of why the majority of acti...  \n",
       "2  first of all hate those moronic rapper who cou...  \n",
       "3  not even the beatles could write song everyone...  \n",
       "4  brass picture movie is not fitting word for th...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Processed_Reviews'] = texts_processed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the data:  (25000, 4)\n",
      "Dimension of the train data:  (25000, 4)\n"
     ]
    }
   ],
   "source": [
    "df_test = df.loc[df['type'] == 'test']\n",
    "print(\"Dimension of the data: \", df_test.shape)\n",
    "\n",
    "df_train = df.loc[df['type'] == 'train']\n",
    "print(\"Dimension of the train data: \", df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tokenized text into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Datasets/texts_tokenized.txt', 'wb') as fp:\n",
    "    pickle.dump(texts_tokenized, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Dataframe into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('Datasets/imdb_master_train.csv')\n",
    "df_test.to_csv('Datasets/imdb_master_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
