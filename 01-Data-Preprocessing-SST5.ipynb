{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/sst_train.txt', sep='\\t', header=None, names=['label', 'review'], encoding=\"latin-1\")\n",
    "df['label'] = df['label'].str.replace('__label__', '')\n",
    "df['label'] = df['label'].astype(int).astype('category')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv('Datasets/sst_dev.txt', sep='\\t', header=None, names=['label', 'review'], encoding=\"latin-1\")\n",
    "df_val['label'] = df_val['label'].str.replace('__label__', '')\n",
    "df_val['label'] = df_val['label'].astype(int).astype('category')\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('Datasets/sst_test.txt', sep='\\t', header=None, names=['label', 'review'], encoding=\"latin-1\")\n",
    "df_test['label'] = df_test['label'].str.replace('__label__', '')\n",
    "df_test['label'] = df_test['label'].astype(int).astype('category')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting a list of sentences to a list of lists containing tokenized words\n",
    "def docs_preprocessor(inputDocs):\n",
    "    docs = inputDocs.copy()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenize the words.\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    #docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Convert a list of sentences to a list of lists containing tokenized words\n",
    "texts_tokenized = docs_preprocessor(df[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bigrams/Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Add bigrams to docs (only ones that appear 10 times or more).\n",
    "'''\n",
    "bigram = Phrases(texts_tokenized, min_count=10, threshold=0.5, scoring='npmi')\n",
    "#trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(texts_tokenized)):\n",
    "    for token in bigram[texts_tokenized[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            texts_tokenized[idx].append(token)\n",
    "#     for token in trigram[texts_tokenized[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_tokenized[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tokenized_counts = defaultdict(int)\n",
    "\n",
    "for row in texts_tokenized:\n",
    "    for word in row:\n",
    "            texts_tokenized_counts[word] += 1\n",
    "vocabulary = list(texts_tokenized_counts.keys())\n",
    "\n",
    "print(\"\\nVocabulary size: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases Count\n",
    "\n",
    "We can investigate the phrases (by uncommenting the print statement below) and, if needed, increase/decrease the number of phrases by varying the \"min_count\" and \"threshold\" parameters of the Phrases object (above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find phrases that are joined by an underscore (_)\n",
    "numOfPhrases = 0\n",
    "for i in range(len(vocabulary)):\n",
    "    if(vocabulary[i].find(\"_\") > -1):\n",
    "        numOfPhrases += 1\n",
    "        #print(vocabulary[i])\n",
    "    \n",
    "print(\"\\nTotal Number of Phrases: \", numOfPhrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Tokens Count\n",
    "The Keras tokenizer needs us to set the number of high frequency tokens/words. These top k tokens will be used to define the length of the feature vectors.\n",
    "\n",
    "Thus, we need to understand how many words have high frequency. Analyzing the frequency of the tokens we can decide the threshold for the high frequency tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of ALL tokens\n",
    "tokens = []\n",
    "for text in texts_tokenized:\n",
    "    for token in text:\n",
    "        tokens.append(token)\n",
    "        \n",
    "print(\"Total number of tokens (including repetition): \", len(tokens))\n",
    "\n",
    "\n",
    "# Sort tokens from high to low frequency\n",
    "token_frequency = Counter(tokens)\n",
    "token_frequency_ordered = OrderedDict(sorted(token_frequency.items(), key=lambda t: t[1], reverse = True))\n",
    "\n",
    "\n",
    "'''\n",
    "We may view the top frequent tokens (by uncommenting the print statement below).\n",
    "By varying the \"top_k_tokens\" we can see the top k tokens.\n",
    "This will help to determine the threshold for top k frequent tokens to create the vectors.\n",
    "'''\n",
    "numOfTokens = 0\n",
    "top_k_tokens = 10000\n",
    "for k, v in token_frequency_ordered.items(): \n",
    "    if k in token_frequency_ordered.keys():\n",
    "        numOfTokens += 1\n",
    "        #print(k, v)\n",
    "        if(numOfTokens == top_k_tokens):\n",
    "            break# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove underscores from Phrases\n",
    "\n",
    "The bigrams are created by adding an underscore between two words.\n",
    "\n",
    "We remove the underscore from all words. Otherwise later the Keras tokenizer will split the phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts_tokenized)):\n",
    "    for j in range(len(texts_tokenized[i])):\n",
    "        texts_tokenized[i][j] = texts_tokenized[i][j].replace(\"_\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text corpus by adding Phrases\n",
    "\n",
    "Combine the words, including the phrases to create a full text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_processed = []\n",
    "\n",
    "for i in range(len(texts_tokenized)):\n",
    "    text = \" \".join(texts_tokenized[i])\n",
    "    texts_processed.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first two reviews in the text corpus\n",
    "texts_processed[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add processed Reviews from Corpus into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed_Reviews'] = texts_processed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.loc[df['type'] == 'test']\n",
    "print(\"Dimension of the data: \", df_test.shape)\n",
    "\n",
    "df_train = df.loc[df['type'] == 'train']\n",
    "print(\"Dimension of the train data: \", df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tokenized text into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Datasets/texts_tokenized.txt', 'wb') as fp:\n",
    "    pickle.dump(texts_tokenized, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Dataframe into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('Datasets/imdb_master_train.csv')\n",
    "df_test.to_csv('Datasets/imdb_master_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
