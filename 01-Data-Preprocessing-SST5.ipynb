{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0     4  The Rock is destined to be the 21st Century 's...\n",
       "1     5  The gorgeously elaborate continuation of `` Th...\n",
       "2     4  Singer/composer Bryan Adams contributes a slew...\n",
       "3     3  You 'd think by now America would have had eno...\n",
       "4     4               Yet the act is still charming here ."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Datasets/sst_train.txt', sep='\\t', header=None, names=['label', 'review'], encoding=\"latin-1\")\n",
    "df['label'] = df['label'].str.replace('__label__', '')\n",
    "df['label'] = df['label'].astype(int).astype('category')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>It 's a lovely film with lovely performances b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No one goes unindicted here , which is probabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>And if you 're not nearly moved to tears by a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>A warm , funny , engaging film .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Uses sharp humor and insight into human nature...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0     4  It 's a lovely film with lovely performances b...\n",
       "1     3  No one goes unindicted here , which is probabl...\n",
       "2     4  And if you 're not nearly moved to tears by a ...\n",
       "3     5                   A warm , funny , engaging film .\n",
       "4     5  Uses sharp humor and insight into human nature..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val = pd.read_csv('Datasets/sst_dev.txt', sep='\\t', header=None, names=['label', 'review'], encoding=\"latin-1\")\n",
    "df_val['label'] = df_val['label'].str.replace('__label__', '')\n",
    "df_val['label'] = df_val['label'].astype(int).astype('category')\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                             review\n",
       "0     3                     Effective but too-tepid biopic\n",
       "1     4  If you sometimes like to go to the movies to h...\n",
       "2     5  Emerges as something rare , an issue movie tha...\n",
       "3     3  The film provides some great insight into the ...\n",
       "4     5  Offers that rare combination of entertainment ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('Datasets/sst_test.txt', sep='\\t', header=None, names=['label', 'review'], encoding=\"latin-1\")\n",
    "df_test['label'] = df_test['label'].str.replace('__label__', '')\n",
    "df_test['label'] = df_test['label'].astype(int).astype('category')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for converting a list of sentences to a list of lists containing tokenized words\n",
    "def docs_preprocessor(inputDocs):\n",
    "    docs = inputDocs.copy()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # Tokenize the words.\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    # Remove numbers, but not words that contain numbers.\n",
    "    #docs = [[token for token in doc if not token.isdigit()] for doc in docs]\n",
    "    \n",
    "    # Remove words that are only one character.\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "    \n",
    "    # Lemmatize all words in documents.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "  \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Convert a list of sentences to a list of lists containing tokenized words\n",
    "texts_tokenized = docs_preprocessor(df[\"review\"])\n",
    "texts_tokenized_val = docs_preprocessor(df_val[\"review\"])\n",
    "texts_tokenized_test = docs_preprocessor(df_test[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Bigrams/Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "'''\n",
    "Add bigrams to docs (only ones that appear 10 times or more).\n",
    "'''\n",
    "bigram = Phrases(texts_tokenized, min_count=10, threshold=0.5, scoring='npmi')\n",
    "#trigram = Phrases(bigram[docs])\n",
    "\n",
    "for idx in range(len(texts_tokenized)):\n",
    "    for token in bigram[texts_tokenized[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            texts_tokenized[idx].append(token)\n",
    "#     for token in trigram[texts_tokenized[idx]]:\n",
    "#         if '_' in token:\n",
    "#             # Token is a bigram, add to document.\n",
    "#             texts_tokenized[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_tokenized_counts = defaultdict(int)\n",
    "\n",
    "for row in texts_tokenized:\n",
    "    for word in row:\n",
    "            texts_tokenized_counts[word] += 1\n",
    "vocabulary = list(texts_tokenized_counts.keys())\n",
    "\n",
    "print(\"\\nVocabulary size: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases Count\n",
    "\n",
    "We can investigate the phrases (by uncommenting the print statement below) and, if needed, increase/decrease the number of phrases by varying the \"min_count\" and \"threshold\" parameters of the Phrases object (above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find phrases that are joined by an underscore (_)\n",
    "numOfPhrases = 0\n",
    "for i in range(len(vocabulary)):\n",
    "    if(vocabulary[i].find(\"_\") > -1):\n",
    "        numOfPhrases += 1\n",
    "        #print(vocabulary[i])\n",
    "    \n",
    "print(\"\\nTotal Number of Phrases: \", numOfPhrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Tokens Count\n",
    "The Keras tokenizer needs us to set the number of high frequency tokens/words. These top k tokens will be used to define the length of the feature vectors.\n",
    "\n",
    "Thus, we need to understand how many words have high frequency. Analyzing the frequency of the tokens we can decide the threshold for the high frequency tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of ALL tokens\n",
    "tokens = []\n",
    "for text in texts_tokenized:\n",
    "    for token in text:\n",
    "        tokens.append(token)\n",
    "        \n",
    "print(\"Total number of tokens (including repetition): \", len(tokens))\n",
    "\n",
    "\n",
    "# Sort tokens from high to low frequency\n",
    "token_frequency = Counter(tokens)\n",
    "token_frequency_ordered = OrderedDict(sorted(token_frequency.items(), key=lambda t: t[1], reverse = True))\n",
    "\n",
    "\n",
    "'''\n",
    "We may view the top frequent tokens (by uncommenting the print statement below).\n",
    "By varying the \"top_k_tokens\" we can see the top k tokens.\n",
    "This will help to determine the threshold for top k frequent tokens to create the vectors.\n",
    "'''\n",
    "numOfTokens = 0\n",
    "top_k_tokens = 10000\n",
    "for k, v in token_frequency_ordered.items(): \n",
    "    if k in token_frequency_ordered.keys():\n",
    "        numOfTokens += 1\n",
    "        #print(k, v)\n",
    "        if(numOfTokens == top_k_tokens):\n",
    "            break# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove underscores from Phrases\n",
    "\n",
    "The bigrams are created by adding an underscore between two words.\n",
    "\n",
    "We remove the underscore from all words. Otherwise later the Keras tokenizer will split the phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts_tokenized)):\n",
    "    for j in range(len(texts_tokenized[i])):\n",
    "        texts_tokenized[i][j] = texts_tokenized[i][j].replace(\"_\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create text corpus by adding Phrases\n",
    "\n",
    "Combine the words, including the phrases to create a full text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_processed = []\n",
    "\n",
    "for i in range(len(texts_tokenized)):\n",
    "    text = \" \".join(texts_tokenized[i])\n",
    "    texts_processed.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first two reviews in the text corpus\n",
    "texts_processed[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add processed Reviews from Corpus into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed_Reviews'] = texts_processed\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.loc[df['type'] == 'test']\n",
    "print(\"Dimension of the data: \", df_test.shape)\n",
    "\n",
    "df_train = df.loc[df['type'] == 'train']\n",
    "print(\"Dimension of the train data: \", df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tokenized text into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Datasets/texts_tokenized.txt', 'wb') as fp:\n",
    "    pickle.dump(texts_tokenized, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Dataframe into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('Datasets/imdb_master_train.csv')\n",
    "df_test.to_csv('Datasets/imdb_master_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
